---
title : '[Paper] DETR : End-to-End Object Detection with Transformers'
date : 2022-08-31
categories : 'deeplearning'
tags : ['object detection', 'deep learning'] #['tags1', 'tags2']
toc : true
toc_sticky : true
---

# Abstract
- Object Detection 문제를 Direct Set Prediction Problem으로 접근 → detection pipeline을 이전보다 간소화
  - **세부적으로는 NMS (Non-Maximum Suppression)이나 Anchor Generation과 같은 hand designed 요소들을 제거**했음
- 모델의 주요 요소 : 이분 매칭 (bipartite matching), set-based global loss, Transformer Encoder-Decoder 아키텍처
- 모델 자체가 개념적으로 매우 단순하기 때문에 구현과 (Panoptic Segmentation과 같은) 다른 분야로의 응용이 굉장히 쉬움.
- COCO object detection 데이터셋을 기준으로, Faster R-CNN과 수준의 성능을 보임.

# Introduction
- DETR의 Main Feature
  - 이분 매칭과 Parrael decoding Transformer의 결합
  - 이분 매칭 알고리즘이 예측 결과를 ground truth object에 uniquely assign 해주기 때문에, 예측 결과 중 나타나는 object들의 순서에 영향을 받지 않는다.
    - 따라서 DETR은 병렬적으로 예측을 수행하는 것이 가능하다.
    - 병렬적으로 수행한다는 의미는, 기존의 RNN이나 auto-regressive Transformer(= Attention Is All You Need에서 제안한 초기 버전의 Transformer)처럼 결과를 하나씩 출력하지 않고, 모든 결과를 한번에 출력한다는 의미
- Transformer 기반의 모델이기 때문에 역시 매우 긴 훈련 시간을 필요로 하며 (extra-long training schedule)
- 디코딩 레이어에서의 보조 손실 함수 (auxiliary decoding loss)를 통해 이득을 얻을 수 있다

# Related Work

## Set Predciton
- 이전까지는 직접적으로 집합을 예측하는 표준적인 딥러닝 모델은 존재하지 않았음
- 기존에는 인접-중복 (near-duplicate) 문제를 NMS와 같은 후처리를 통해서 제거 BUT direct set prediction은 후처리 필요 X
- Direct Set Prediction을 위해서는 모든 predicted element를 다루는 global inference 계획이 필요하다
  - 1) Fully Connected Layer → 충분하지만 연산량이 많이 듦
  - 2) Auto-regressive Sequence Model (e.g. RNN)
- 무엇을 쓰던지 간에 loss는 예측 결과의 순서에 영향을 받지 않아야 한다.  보통 이를 위해 쓰는 방법이 헝가리안 알고리즘에 기반을 둔 이분 매칭을 통해 ground-truth와 예측 결과를 서로 매칭하는 방법이다.
- 논문에서는 Auto-Regressive 모델에서 한발 더 나아가, Transformer 구조를 이용한다

## Transformers and Parallel Decoding
- Attention의 장점
  - Global Computation, Perfect Memory
    - → Long Sequence 기준 RNN보다 적합
    - NLP, Speech-processing, Vision 등 여러 분야에서 RNN을 빠르게 대체하는 중
- 초기 Transformer는 Auto-regressive 모델
  - output length가 길어질수록 덩달아 추론 시간도 비례해서 늘어난다는 단점
  - 이를 해결하기 위해 오디오, 기계 번역, 단어 표현 학습, 음성 인식 등 분야에서 parallel decoding Transformer 모델이 등장
- 우리도 그래서 parallel Transformer 쓸꺼다~

## Object Detection
- Two Stage Detector든, One Stage Detector 모델이든 **Initial Guess**가 존재
  - Two Stage Detector : Proposal을 통해 box를 예측
  - One Stage Detector : Anchor 또는 가능한 물체 중심에 대한 Grid를 이용헤 예측을 수행
- 최근의 연구 결과에 따르면 모델의 최종 성능은 이러한 Initial Guess에 의해 상당한 영향을 받는다고 함 (Zhang,S.,Chi,C.,Yao,Y.,Lei,Z.,Li,S.Z.:Bridging the gap between anchor-based
and anchor-free detection via adaptive training sample selection, 2019)
- 그래서 논문에서는 이러한 것들을 (hand-crafted process) 싹 다 제거하고 이미지 기반으로만 예측을 수행
- **이분 매칭 + Parallel Decoding Transformer 조합 + non hand-crafted feature 조합**은 이 논문이 최초

# The DETR model

## Object Detection set prediction loss
- DETR은 디코더를 한번만 통과하여, N이라는 고정된 크기의 prediction 결과를 도출한다.
  - 이때, N은 이미지에 있는 일반적인 물체의 개수보다 훨씬 큰 값이다.
- 문제 : 예측 결과에 대해 ground truth와 관련 있는 만큼 점수 매기기

$$ y = \{y1, y2, y3, ...,  \varnothing \}$$
$$ \hat{y} = \{\hat{y1}, \hat{y2}, \hat{y3}, ...,  \varnothing \}$$

$ \varnothing $ : no object 


![DETR Fig.2](https://i.imgur.com/v10IWwM.png)